from tabnanny import check
import pandas as pd
import re
import requests
from typing import Optional
from fastapi import FastAPI, HTTPException, Request
# регестрация пажилаю собсна
class BearerAuth(requests.auth.AuthBase):
    def __init__(self, token):
        self.token = token
    def __call__(self, r):
        r.headers["authorization"] = "Bearer " + self.token
        return r

data = pd.read_csv('Pajiloy.csv')

def preprocess(text):
  text_input = re.sub('[^a-zA-Z1-9]+', ' ', str(text))
  output = re.sub(r'\d+', '',text_input)
  return output.lower().strip()

data['review'] = data.review.map(preprocess)
corpus_cleaned = data['review'].astype(str).values.tolist()

# for siplicity, we keep only reviews with 12 words
sentences = []
sentence_length = []

for item in corpus_cleaned:
  word_list = item.split()
  sentences.append(item)
  number_of_words = len(word_list)
  sentence_length.append(number_of_words)

sentences = pd.DataFrame(sentences)
sentence_length = pd.DataFrame(sentence_length)

sentences = sentences.merge(sentence_length, left_index=True, right_index=True)

sentences['number_of_words']=sentences['0_y']
sentences['sentence']=sentences['0_x']
sentences=sentences[['sentence','number_of_words']]
sentences['number_of_words']=sentences['number_of_words'].astype(int)

sentences = sentences[sentences['number_of_words'] <= 12]
corpus_cleaned=sentences['sentence'].values.tolist()
corpus = corpus_cleaned

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(corpus)

# create input sequences using list of tokens
input_sequences = []

for review in corpus:
  token_list = tokenizer.texts_to_sequences([review])[0]
  for i in range(1, len(token_list)):
    n_gram_sequence = token_list[:i+1]
    input_sequences.append(n_gram_sequence)
  
import tensorflow.keras.utils as ku
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# pad sequences
max_sequence_len = max([len(x) for x in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))

# create predictors and label
total_words = len(tokenizer.word_index) + 1
predictors, label = input_sequences[:,:-1],input_sequences[:,-1]
label = ku.to_categorical(label, num_classes=total_words)

from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras import regularizers
import tensorflow as tf

model = Sequential()
model.add(Embedding(total_words, 240, input_length=max_sequence_len-1))
model.add(Bidirectional(LSTM(150, return_sequences = True)))
model.add(Dropout(0.2))
model.add(LSTM(100))
model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(Dense(total_words, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.93):
      print("\nReached 93% accuracy so cancelling training!")
      self.model.stop_training = True

callbacks = myCallback()

history = model.fit(predictors, label, epochs=1, verbose=1, callbacks=[callbacks])

# import matplotlib.pyplot as plt
# from matplotlib.pyplot import figure

# acc = history.history['accuracy']
# loss = history.history['loss']

# epochs = range(len(acc))

# figure(figsize=(14, 7), dpi=80)

# plt.subplot(1,2,1)
# plt.plot(epochs, acc, 'b', label='Training accuracy')
# plt.title('Training accuracy')
# plt.legend()

# plt.subplot(1,2,2)
# plt.plot(epochs, loss, 'b', label='Training Loss')
# plt.title('Training loss')
# plt.legend()


# plt.show()
from pydantic import BaseModel, ValidationError

class OurBaseModel(BaseModel):
    class Config:
        orm_mode = True
    
app = FastAPI()

class Item(OurBaseModel):
    name: str
    
    
@app.post("/items/",
         )
def CreateSentence(item: Item):
      seed_text = item.name
      if seed_text != "stop":
        next_words = 50
        for _ in range(next_words):
            token_list = tokenizer.texts_to_sequences([seed_text])[0]
            token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
            predict_x=model.predict(token_list) 
            predicted=np.argmax(predict_x,axis=1)
            # predicted = model.predict_classes(token_list, verbose=0)
            output_word = ""
            for word, index in tokenizer.word_index.items():
                if index == predicted:
                    output_word = word
                    break
            seed_text += " " + output_word

        seed_text = (seed_text + ".").capitalize()
        print(seed_text)
        return seed_text

@app.get("/")
def read_root():
    return CreateSentence
 # seed_text = requests.get('https://localhost:3000/', auth=BearerAuth('eyJpc3MiOiJPbmxpbmUgSldUIEJ1aWxkZXIiLCJpYXQiOjE2NTE0MzYwMzMsImV4cCI6MTY4MzE0NDgzMywiYXVkIjoid3d3LmV4YW1wbGUuY29tIiwic3ViIjoianJvY2tldEBleGFtcGxlLmNvbSIsIkdpdmVuTmFtZSI6IkpvaG5ueSIsIlN1cm5hbWUiOiJSb2NrZXQiLCJFbWFpbCI6Impyb2NrZXRAZXhhbXBsZS5jb20iLCJSb2xlIjpbIk1hbmFnZXIiLCJQcm9qZWN0IEFkbWluaXN0cmF0b3IiXX0'))   
  # def SendAnswer(seed_text):
  #       endpoint = "https://localhost:3000/"
  #       data = {seed_text}
  #       requests.post(endpoint, data=data, auth=BearerAuth('eyJpc3MiOiJPbmxpbmUgSldUIEJ1aWxkZXIiLCJpYXQiOjE2NTE0MzYwMzMsImV4cCI6MTY4MzE0NDgzMywiYXVkIjoid3d3LmV4YW1wbGUuY29tIiwic3ViIjoianJvY2tldEBleGFtcGxlLmNvbSIsIkdpdmVuTmFtZSI6IkpvaG5ueSIsIlN1cm5hbWUiOiJSb2NrZXQiLCJFbWFpbCI6Impyb2NrZXRAZXhhbXBsZS5jb20iLCJSb2xlIjpbIk1hbmFnZXIiLCJQcm9qZWN0IEFkbWluaXN0cmF0b3IiXX0')).json()
      
